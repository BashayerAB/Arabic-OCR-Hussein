{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOGAkNfcictUpWQ7TybBBan",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f26c9a782e40417caa4056c29e8a6da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dbb0b11dab7413983e9c04ecc7ae8be",
              "IPY_MODEL_6fbccced5c0a41ada65fdbe4063d747a",
              "IPY_MODEL_380ac176dfe9475ca01294a38a0a8e60"
            ],
            "layout": "IPY_MODEL_6f9b930a6aed45e5ac0dcb419a9e22f1"
          }
        },
        "6dbb0b11dab7413983e9c04ecc7ae8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_701301f1f48a47d7b4ee651dab55e214",
            "placeholder": "​",
            "style": "IPY_MODEL_90e7db91cf7a4f3096872ec77875aa73",
            "value": "100%"
          }
        },
        "6fbccced5c0a41ada65fdbe4063d747a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74cd5082886d4bdc9f53d5eb6748739c",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e1248b3c10c47d8b5bd10bc1046a041",
            "value": 30
          }
        },
        "380ac176dfe9475ca01294a38a0a8e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0009f88809e94bba9e5596edb165ebf8",
            "placeholder": "​",
            "style": "IPY_MODEL_aeac5d0ee49947668d8547239273cb65",
            "value": " 30/30 [59:59&lt;00:00, 119.86s/it]"
          }
        },
        "6f9b930a6aed45e5ac0dcb419a9e22f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "701301f1f48a47d7b4ee651dab55e214": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e7db91cf7a4f3096872ec77875aa73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74cd5082886d4bdc9f53d5eb6748739c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e1248b3c10c47d8b5bd10bc1046a041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0009f88809e94bba9e5596edb165ebf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeac5d0ee49947668d8547239273cb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BashayerAB/Arabic-OCR-Hussein/blob/master/RNN_EffNet_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hijaa is a dataset for handwritten Arabic letters collected from Arabic-speaking schoolchildren between the ages of 7 and 12. Data were collected in Riyadh, Saudi Arabia, from January to April, 2019. It represents a total of 47,434 characters written by 591 participants in different forms.\n",
        "\n",
        "The dataset is organized into 29 folders, each corresponding to an Arabic letter, with one folder for “hamza”. Each folder contains subfolders of the various letter forms for each letter. Each subfolder contains the images for that particular letter form. Vocalization diacritics that mark vowels and other sounds that cannot be represented by Arabic letters (harakat) are not included in our dataset."
      ],
      "metadata": {
        "id": "NrCYNGkiA13Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, Dict, List\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "from pathlib import Path\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.models as models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "# Lets check Pytorch version\n",
        "#torch.__version__"
      ],
      "metadata": {
        "id": "DNWXoh4VE8ep"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "HXW8xu3dFR7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b032e22f-2829-4076-8844-2c0a802f1ef2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4356, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 4356 (delta 154), reused 119 (delta 119), pack-reused 4171 (from 3)\u001b[K\n",
            "Receiving objects: 100% (4356/4356), 654.37 MiB | 17.25 MiB/s, done.\n",
            "Resolving deltas: 100% (2583/2583), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/daadturki1/Dhad.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ch8ShYEnef-",
        "outputId": "d51fe543-4755-4d1c-9ea2-ab6a3f812cf1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Dhad'...\n",
            "remote: Enumerating objects: 56170, done.\u001b[K\n",
            "remote: Counting objects: 100% (9930/9930), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9918/9918), done.\u001b[K\n",
            "remote: Total 56170 (delta 9), reused 9930 (delta 9), pack-reused 46240 (from 1)\u001b[K\n",
            "Receiving objects: 100% (56170/56170), 48.66 MiB | 16.18 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Updating files: 100% (56075/56075), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JsAjgcTt-J2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HusseinYoussef/Arabic-OCR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqnk8AJLocZx",
        "outputId": "af913f77-c5cf-4ac3-8d5f-5a8b1fad8ffc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arabic-OCR'...\n",
            "remote: Enumerating objects: 272, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 272 (delta 130), reused 245 (delta 121), pack-reused 7 (from 1)\u001b[K\n",
            "Receiving objects: 100% (272/272), 13.04 MiB | 11.45 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Arabic-OCR/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIrbB8s_or4b",
        "outputId": "a8362ee1-a8ac-4372-a0fb-ad4f99c4b592"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arabic-OCR/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Arabic-OCR/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qv1TcoFotkQ",
        "outputId": "ecce24bb-0638-49ef-a3dd-f78f206655a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cycler==0.10.0 (from -r /content/Arabic-OCR/requirements.txt (line 1))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
            "Collecting decorator==5.0.3 (from -r /content/Arabic-OCR/requirements.txt (line 2))\n",
            "  Downloading decorator-5.0.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistance==0.5.3 (from -r /content/Arabic-OCR/requirements.txt (line 3))\n",
            "  Downloading editdistance-0.5.3.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imageio==2.9.0 (from -r /content/Arabic-OCR/requirements.txt (line 4))\n",
            "  Downloading imageio-2.9.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib==1.2.0 (from -r /content/Arabic-OCR/requirements.txt (line 5))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting kiwisolver==1.3.1 (from -r /content/Arabic-OCR/requirements.txt (line 6))\n",
            "  Downloading kiwisolver-1.3.1.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx==2.5 (from -r /content/Arabic-OCR/requirements.txt (line 7))\n",
            "  Downloading networkx-2.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting numpy==1.22.0 (from -r /content/Arabic-OCR/requirements.txt (line 8))\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting opencv-python==4.5.1.48 (from -r /content/Arabic-OCR/requirements.txt (line 9))\n",
            "  Downloading opencv-python-4.5.1.48.tar.gz (88.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HusseinYoussef/Arabic-OCR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a199919e-62fc-4e98-e670-400c52cbf3e1",
        "id": "GyQmuR9R-kaC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arabic-OCR'...\n",
            "remote: Enumerating objects: 272, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 272 (delta 130), reused 245 (delta 121), pack-reused 7 (from 1)\u001b[K\n",
            "Receiving objects: 100% (272/272), 13.04 MiB | 18.35 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Arabic-OCR/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25268400-c90a-4e2f-ffe8-531d6ad035dd",
        "id": "J-bMVju8-kaD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arabic-OCR/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Arabic-OCR/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb4892d-7075-4111-f27c-10764af0cc74",
        "id": "uN-kfrNg-kaE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cycler==0.10.0 (from -r /content/Arabic-OCR/requirements.txt (line 1))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
            "Collecting decorator==5.0.3 (from -r /content/Arabic-OCR/requirements.txt (line 2))\n",
            "  Downloading decorator-5.0.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistance==0.5.3 (from -r /content/Arabic-OCR/requirements.txt (line 3))\n",
            "  Downloading editdistance-0.5.3.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imageio==2.9.0 (from -r /content/Arabic-OCR/requirements.txt (line 4))\n",
            "  Downloading imageio-2.9.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib==1.2.0 (from -r /content/Arabic-OCR/requirements.txt (line 5))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting kiwisolver==1.3.1 (from -r /content/Arabic-OCR/requirements.txt (line 6))\n",
            "  Downloading kiwisolver-1.3.1.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx==2.5 (from -r /content/Arabic-OCR/requirements.txt (line 7))\n",
            "  Downloading networkx-2.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting numpy==1.22.0 (from -r /content/Arabic-OCR/requirements.txt (line 8))\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting opencv-python==4.5.1.48 (from -r /content/Arabic-OCR/requirements.txt (line 9))\n",
            "  Downloading opencv-python-4.5.1.48.tar.gz (88.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HusseinYoussef/Arabic-OCR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a199919e-62fc-4e98-e670-400c52cbf3e1",
        "id": "y6PCrT3j-oKJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arabic-OCR'...\n",
            "remote: Enumerating objects: 272, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 272 (delta 130), reused 245 (delta 121), pack-reused 7 (from 1)\u001b[K\n",
            "Receiving objects: 100% (272/272), 13.04 MiB | 18.35 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Arabic-OCR/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25268400-c90a-4e2f-ffe8-531d6ad035dd",
        "id": "NaW6k_ol-oKL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Arabic-OCR/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/Arabic-OCR/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb4892d-7075-4111-f27c-10764af0cc74",
        "id": "Mss8Ifsr-oKL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cycler==0.10.0 (from -r /content/Arabic-OCR/requirements.txt (line 1))\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n",
            "Collecting decorator==5.0.3 (from -r /content/Arabic-OCR/requirements.txt (line 2))\n",
            "  Downloading decorator-5.0.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistance==0.5.3 (from -r /content/Arabic-OCR/requirements.txt (line 3))\n",
            "  Downloading editdistance-0.5.3.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imageio==2.9.0 (from -r /content/Arabic-OCR/requirements.txt (line 4))\n",
            "  Downloading imageio-2.9.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib==1.2.0 (from -r /content/Arabic-OCR/requirements.txt (line 5))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting kiwisolver==1.3.1 (from -r /content/Arabic-OCR/requirements.txt (line 6))\n",
            "  Downloading kiwisolver-1.3.1.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx==2.5 (from -r /content/Arabic-OCR/requirements.txt (line 7))\n",
            "  Downloading networkx-2.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting numpy==1.22.0 (from -r /content/Arabic-OCR/requirements.txt (line 8))\n",
            "  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting opencv-python==4.5.1.48 (from -r /content/Arabic-OCR/requirements.txt (line 9))\n",
            "  Downloading opencv-python-4.5.1.48.tar.gz (88.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset paths (replace with actual paths)\n",
        "data_dir = \"/content/Dhad/Dhad_Dataset\"  # Replace with your dataset directory\n",
        "train_dir = data_dir + \"/train\"\n",
        "test_dir = data_dir + \"/test\"\n",
        "val_dir = data_dir + \"/validation\"  # Adjust if validation folder name is different\n",
        "\n",
        "# Data transformations (adjust based on your dataset requirements)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),  # Adjust image size as needed\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Adjust normalization if needed\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),  # Adjust image size as needed (may differ from training)\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Same normalization as training\n",
        "    ]),\n",
        "    'val': transforms.Compose([  # Add validation transforms if needed\n",
        "        transforms.Resize(256),  # Adjust image size as needed\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Same normalization as training\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Load datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}  # Adjust batch size, shuffle, and num_workers as needed\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test', 'val']}\n",
        "class_names = image_datasets['train'].classes  # Assuming class labels are in the training folder\n",
        "\n",
        "# Accessing image count per folder (assuming you have the paths defined correctly)\n",
        "train_count = len(image_datasets['train'])\n",
        "test_count = len(image_datasets['test'])\n",
        "val_count = len(image_datasets['val'])  # Adjust if validation folder name is different\n",
        "\n",
        "print(f\"Train images: {train_count}\")\n",
        "print(f\"Test images: {test_count}\")\n",
        "print(f\"Validation images: {val_count}\")  # Adjust if validation folder name is different\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb04f7VwPAy5",
        "outputId": "f2999ad7-a98e-44af-9a0f-0b96462fbe49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 25545\n",
            "Test images: 10333\n",
            "Validation images: 10310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"https://github.com/daadturki1/Dhad/tree/main/Dhad_Dataset\""
      ],
      "metadata": {
        "id": "_2lJC4Roocmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"Dhad/Dhad_Dataset\"  # Path to the dataset folder within the cloned repository\n",
        "train_path = dataset_path + \"/train\"\n",
        "test_path = dataset_path + \"/test\"\n",
        "val_path = dataset_path + \"/val\""
      ],
      "metadata": {
        "id": "hArzlZjEr547"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_directory = \"/content/Dhad/Dhad_Dataset/train\"\n",
        "print(f\"Target dir: {target_directory}\")\n",
        "\n",
        "# Get the class names from the target directory and discard the first index\n",
        "class_names_found = sorted([entry.name for entry in os.scandir(target_directory) if entry.is_dir()])[1:]\n",
        "\n",
        "# Count the number of images in each folder\n",
        "image_counts = {}\n",
        "for class_name in class_names_found:\n",
        "    class_directory = os.path.join(target_directory, class_name)\n",
        "    image_count = len([entry for entry in os.scandir(class_directory) if entry.is_file()])\n",
        "    image_counts[class_name] = image_count\n",
        "\n",
        "# Print the number of images in each folder\n",
        "for class_name, count in image_counts.items():\n",
        "    print(f\"{class_name}: {count} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy1fGQshBPCX",
        "outputId": "20e6413b-e33a-40cf-b828-01ffb7955f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target dir: /content/Dhad/Dhad_Dataset/train\n",
            "10-raa: 538 images\n",
            "11-zeyn: 566 images\n",
            "12-seen: 696 images\n",
            "13-sheen: 784 images\n",
            "14-sad: 598 images\n",
            "15-dhad: 1061 images\n",
            "16-t_aa: 1155 images\n",
            "17-th_aa: 1131 images\n",
            "18-ayen: 1093 images\n",
            "19-ghayen: 600 images\n",
            "2-baa: 1109 images\n",
            "20-faa: 877 images\n",
            "21-ghaf: 1006 images\n",
            "22-kaf: 1211 images\n",
            "23-lam: 710 images\n",
            "24-meem: 1173 images\n",
            "25-noon: 1147 images\n",
            "26-haa: 1308 images\n",
            "27-waw: 523 images\n",
            "28-yaa: 681 images\n",
            "29-hamzah: 844 images\n",
            "3-taa: 977 images\n",
            "4-thaa: 1040 images\n",
            "5-jeem: 972 images\n",
            "6-h_aa: 1152 images\n",
            "7-khaa: 660 images\n",
            "8-dal: 558 images\n",
            "9-thal: 549 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GrayscaleToRGB(object):\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = img.squeeze().cpu().numpy()\n",
        "            img = np.transpose(img, (1, 2, 0))\n",
        "            img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
        "            img = (255 * img).astype(np.uint8)\n",
        "            img = PIL.Image.fromarray(img)\n",
        "\n",
        "        return img.convert('RGB')"
      ],
      "metadata": {
        "id": "dBMjUKFGiK9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "jEEuLl76tYid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageFolder(train_path, transform=transform)\n",
        "test_dataset = ImageFolder(test_path, transform=transform)\n",
        "val_dataset = ImageFolder(val_path, transform=transform)"
      ],
      "metadata": {
        "id": "PXgmT8I4txcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loaderr = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loaderr = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "val_loaderr = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "JUVlFhPzjFFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_index = random.randint(0, len(train_dataset)-1)\n",
        "image, label = train_dataset[random_index]"
      ],
      "metadata": {
        "id": "grWBfbj0wJ2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = image.numpy().transpose((1, 2, 0))"
      ],
      "metadata": {
        "id": "tVpCvkpMwWX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_dataset.classes\n",
        "label_name = class_names[label]\n",
        "plt.imshow(image)\n",
        "plt.title(label_name)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "S2P_J949waTO",
        "outputId": "70299e68-929d-43b4-b1e2-493da9bc6a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image , label = train_dataset.__getitem__(32)\n",
        "\n",
        "image.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef6G5VGF30GU",
        "outputId": "17ab5655-4a3c-471f-f29f-6392e5ab77d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2PMrzty79dX"
      },
      "outputs": [],
      "source": [
        "#train_loader = DataLoader(train_dataset, batch_size= 64, shuffle=True)\n",
        "#test_loader = DataLoader(test_dataset, batch_size= 64, shuffle=False)\n",
        "#val_loader = DataLoader(val_dataset, batch_size= 64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GrayscaleToRGB(object):\n",
        "    def __call__(self, img):\n",
        "        return img.convert('RGB')"
      ],
      "metadata": {
        "id": "1-Fgx1_qh2h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image as ndarray type (Height * Width * Channels)\n",
        "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
        "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
        "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
        "        image_array = self.data.iloc[index, self.data.columns != 'label'].values.astype(np.uint8).reshape(32,32)\n",
        "        image = Image.fromarray(image_array)\n",
        "        label = self.data.iloc[index, -1] - 1\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "7bcVGdAKh_BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Set seed\n",
        "random.seed(42)\n",
        "image_path_organised='/content/Dhad/Dhad_Dataset'\n",
        "# 1. Get all image paths\n",
        "image_path_list = list(Path(image_path_organised).glob(\"*/*/*.png\"))\n",
        "\n",
        "# 2. Pick a random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "print(random_image_path)\n",
        "\n",
        "# 3. Get image class from path name\n",
        "image_class = random_image_path.parent.stem\n",
        "print(image_class)\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata\n",
        "print(f\"Random image path:{random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "f2qNly20j7lC",
        "outputId": "095a974f-0db0-48e4-d4ba-49ba4205b8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Dhad/Dhad_Dataset/test/7-khaa/khaa564.png\n",
            "7-khaa\n",
            "Random image path:/content/Dhad/Dhad_Dataset/test/7-khaa/khaa564.png\n",
            "Image class: 7-khaa\n",
            "Image height: 32\n",
            "Image width: 32\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABIUlEQVR4Ae1UMQ5EUBDdv7YRwhH03ED03MNmryF7D6UjUGmUCkruoFBoJWJnjSzxd2N+obKTkPnPmzfy/vzPxnG8HBnXI8Xf2v8Guw6f0CLP8yzL2nVmIcA5oIdpmlhJL7nQqbZtg7phGHVd06uoDfq+B/WyLOnSyKQ2GIYhjmNRdeAzeJYNOSA74TkQdfE8FsGY3qcQtYg6poyxjXTXdZqmbUB+KbYHjylQRdd16PqcYq0LgKqq8J5B4uHEq0JRFOTD769Fv+bInC1K09RxHFmWeWrTNHBJAJ5lWRRFPGGD+L4PSBiGiM8NeIs3Zb+WVVXBp881ztNuCAVBkOc55m3bFkXhuu6aLUlSkiRrhJhTp4gox9PEpoiv30X+DU5g0QvQnORfZOAseAAAAABJRU5ErkJggg==\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iis/XNIi17R59MnnnhguNqymByjMgYFkyOQGAKnHZjQBatLu2v7ZbmzuIbiB87ZYXDq2Dg4I44II/Cpqp6VpVjoel2+m6ZbJbWduu2KJOijOT7kkkkk8kkmrlAGNeeLvDem3stlf+INLtLqIgSQ3F3HG65AYZDEHoQfxri/iJ8TbLTPDRm8L+JNDmvxKhdROk7+V/FsUHBfpwxGRuxziu7vfD2ialK8t/o+n3UjjDPPbI5YehJHNLa+H9FsbhLi00iwt50ztlitkRlyMHBAz0JFAHkmi/HK9m1VtOl0hPEJx5guPD0E4KpuC4MMqhtw65DY+ZR6mu70LxtqGv63DaReDddsbAo5mvdTiW38tgOAEJJYHgZB/DAJHY0UAf/Z\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to visualize and image with matplotlib.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Turn the image into an array\n",
        "img_as_array = np.asarray(img)\n",
        "\n",
        "# Plot the image with matplotlib\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(img_as_array)\n",
        "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> (height, width, color channels) (HWC)\")\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "64_hAJq3kARx",
        "outputId": "437c5673-4256-489b-dd63-303f82eacd15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 31.5, 31.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    transforms.Resize(size=(64,64)), ## this is added in the new code\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "t4AegyU8juLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can transform our random image and print out the dtype, shape and the img representation (tensor)\n",
        "data_transform(img).dtype,data_transform(img).shape, data_transform(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quRdDwGyjpgu",
        "outputId": "37666150-ad29-4ea5-daf1-dc39917a2a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.float32,\n",
              " torch.Size([3, 64, 64]),\n",
              " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              " \n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_transformed_images(image_paths: list, transform, n=3, seed=None):\n",
        "  \"\"\"\n",
        "  Selects random images from a path of images and loads/transforms\n",
        "  them then plots the original vs the transformed version.\n",
        "  \"\"\"\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "  random_image_paths = random.sample(image_paths, k=n)\n",
        "  for image_path in random_image_paths:\n",
        "    with Image.open(image_path) as f:\n",
        "      fig, ax = plt.subplots(nrows=1, ncols=2)\n",
        "      ax[0].imshow(f)\n",
        "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
        "      ax[0].axis(False)\n",
        "\n",
        "      # Transform and plot target image\n",
        "      transformed_image = transform(f).permute(1, 2, 0) # note we will need to change shape for matplotlib (C, H, W) -> (H, W, C)\n",
        "      ax[1].imshow(transformed_image)\n",
        "      ax[1].set_title(f\"Transformed\\nShape: {transformed_image.shape}\")\n",
        "      ax[1].axis(\"off\")\n",
        "\n",
        "      fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
        "\n",
        "plot_transformed_images(image_paths=image_path_list,\n",
        "                        transform=data_transform,\n",
        "                        n=3,\n",
        "                        seed=28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uCLeEiB3jkj_",
        "outputId": "44b166c5-9e34-41f9-86f4-70bdd585bdc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path for target directory\n",
        "target_directory = train_dir\n",
        "print(f\"Target dir: {target_directory}\")\n",
        "\n",
        "# Get the class names from the target directory\n",
        "class_names_found = sorted([entry.name for entry in list(os.scandir(target_directory))])\n",
        "class_names_found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM8WB6wQmMjv",
        "outputId": "8ce086d0-e566-4139-a41d-b57a60e3cb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target dir: /content/Dhad/Dhad_Dataset/train\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store',\n",
              " '1-alif',\n",
              " '10-raa',\n",
              " '11-zeyn',\n",
              " '12-seen',\n",
              " '13-sheen',\n",
              " '14-sad',\n",
              " '15-dhad',\n",
              " '16-t_aa',\n",
              " '17-th_aa',\n",
              " '18-ayen',\n",
              " '19-ghayen',\n",
              " '2-baa',\n",
              " '20-faa',\n",
              " '21-ghaf',\n",
              " '22-kaf',\n",
              " '23-lam',\n",
              " '24-meem',\n",
              " '25-noon',\n",
              " '26-haa',\n",
              " '27-waw',\n",
              " '28-yaa',\n",
              " '29-hamzah',\n",
              " '3-taa',\n",
              " '4-thaa',\n",
              " '5-jeem',\n",
              " '6-h_aa',\n",
              " '7-khaa',\n",
              " '8-dal',\n",
              " '9-thal']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def order_class_list(class_list):\n",
        "    # Remove the first item ('.DS_Store') if it exists\n",
        "    if class_list[0] == '.DS_Store':\n",
        "        class_list = class_list[1:]\n",
        "\n",
        "    # Extract the numbers from class names and convert them to integers\n",
        "    class_numbers = [int(item.split('-')[0]) for item in class_list]\n",
        "\n",
        "    # Sort the class names based on the extracted numbers\n",
        "    ordered_class_list = [class_name for _, class_name in sorted(zip(class_numbers, class_list))]\n",
        "\n",
        "    return ordered_class_list\n",
        "\n",
        "# Example usage:\n",
        "ordered_class_list = order_class_list(class_names_found)\n",
        "print(ordered_class_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tpn4yxSpQwr",
        "outputId": "ec810ba0-1444-4130-b972-333478b82cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1-alif', '2-baa', '3-taa', '4-thaa', '5-jeem', '6-h_aa', '7-khaa', '8-dal', '9-thal', '10-raa', '11-zeyn', '12-seen', '13-sheen', '14-sad', '15-dhad', '16-t_aa', '17-th_aa', '18-ayen', '19-ghayen', '20-faa', '21-ghaf', '22-kaf', '23-lam', '24-meem', '25-noon', '26-haa', '27-waw', '28-yaa', '29-hamzah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "ordered_class_list = order_class_list(class_names_found)\n",
        "print(ordered_class_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54foQ3Kn9PCF",
        "outputId": "7c7c9416-0cb0-44e0-ebf0-e40a7cf7a0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1-alif', '2-baa', '3-taa', '4-thaa', '5-jeem', '6-h_aa', '7-khaa', '8-dal', '9-thal', '10-raa', '11-zeyn', '12-seen', '13-sheen', '14-sad', '15-dhad', '16-t_aa', '17-th_aa', '18-ayen', '19-ghayen', '20-faa', '21-ghaf', '22-kaf', '23-lam', '24-meem', '25-noon', '26-haa', '27-waw', '28-yaa', '29-hamzah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def order_class_list_1(class_list):\n",
        "    # Extract the numbers from class names and convert them to integers\n",
        "    class_numbers = [int(item.split('-')[0]) for item in class_list[1:]]  # Skip the first item\n",
        "    # Sort the class names based on the extracted numbers\n",
        "    ordered_class_list = [class_name for _, class_name in sorted(zip(class_numbers, class_list[1:]))]  # Skip the first item\n",
        "    # Add the first item back to the ordered list\n",
        "    ordered_class_list.insert(0, class_list[0])\n",
        "    return ordered_class_list\n",
        "\n",
        "# Example usage:\n",
        "ordered_class_list_1 = order_class_list_1(class_names_found)\n",
        "print(ordered_class_list_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex-KsmbZqJjC",
        "outputId": "ca02f689-1a4f-418d-82ea-15b3b622d3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.DS_Store', '1-alif', '2-baa', '3-taa', '4-thaa', '5-jeem', '6-h_aa', '7-khaa', '8-dal', '9-thal', '10-raa', '11-zeyn', '12-seen', '13-sheen', '14-sad', '15-dhad', '16-t_aa', '17-th_aa', '18-ayen', '19-ghayen', '20-faa', '21-ghaf', '22-kaf', '23-lam', '24-meem', '25-noon', '26-haa', '27-waw', '28-yaa', '29-hamzah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Tuple\n",
        "import os\n",
        "import re\n",
        "\n",
        "def find_classes_with_indices(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "    \"\"\"Finds the class folder names in a target directory and assigns indices based on numerical order\"\"\"\n",
        "\n",
        "    # 1. Get the class names by scanning the target directory\n",
        "    classes = [entry.name for entry in os.scandir(directory) if entry.is_dir()]\n",
        "\n",
        "    # 2. Raise an error if class names could not be found\n",
        "    if not classes:\n",
        "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}... Please check file structure\")\n",
        "\n",
        "    # 3. Sort the class names based on numerical order\n",
        "    classes.sort(key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
        "\n",
        "    # 4. Create a dictionary of index labels\n",
        "    class_to_idx = {class_name: i for i, class_name in enumerate(classes, start=1)}  # Start index from 1\n",
        "\n",
        "    return classes, class_to_idx\n",
        "\n",
        "# Example usage:\n",
        "target_directory = \"/content/Dhad/Dhad_Dataset/train\"\n",
        "\n",
        "classes, class_to_idx = find_classes_with_indices(target_directory)\n",
        "\n",
        "# Print the classes and their corresponding indices\n",
        "for class_name, index in class_to_idx.items():\n",
        "    print(f\"Class: {class_name}, Index: {index}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJTZoP-Z_I5r",
        "outputId": "e593f222-052d-4f0a-85a6-46dfe531149f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: 1-alif, Index: 1\n",
            "Class: 2-baa, Index: 2\n",
            "Class: 3-taa, Index: 3\n",
            "Class: 4-thaa, Index: 4\n",
            "Class: 5-jeem, Index: 5\n",
            "Class: 6-h_aa, Index: 6\n",
            "Class: 7-khaa, Index: 7\n",
            "Class: 8-dal, Index: 8\n",
            "Class: 9-thal, Index: 9\n",
            "Class: 10-raa, Index: 10\n",
            "Class: 11-zeyn, Index: 11\n",
            "Class: 12-seen, Index: 12\n",
            "Class: 13-sheen, Index: 13\n",
            "Class: 14-sad, Index: 14\n",
            "Class: 15-dhad, Index: 15\n",
            "Class: 16-t_aa, Index: 16\n",
            "Class: 17-th_aa, Index: 17\n",
            "Class: 18-ayen, Index: 18\n",
            "Class: 19-ghayen, Index: 19\n",
            "Class: 20-faa, Index: 20\n",
            "Class: 21-ghaf, Index: 21\n",
            "Class: 22-kaf, Index: 22\n",
            "Class: 23-lam, Index: 23\n",
            "Class: 24-meem, Index: 24\n",
            "Class: 25-noon, Index: 25\n",
            "Class: 26-haa, Index: 26\n",
            "Class: 27-waw, Index: 27\n",
            "Class: 28-yaa, Index: 28\n",
            "Class: 29-hamzah, Index: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. Write a custom dataset class\n",
        "# 1. Subclass torch.utils.data.Dataset\n",
        "class ArabicHCDataset(Dataset):\n",
        "  # 2. Initialize our custom dataset\n",
        "  def __init__(self,\n",
        "               targ_dir: str,\n",
        "               transform=None):\n",
        "    # 3. Create class attributes\n",
        "    # Get all of the image paths\n",
        "    self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.png\"))\n",
        "    # Setup transform\n",
        "    self.transform = transform\n",
        "    # Create classes and class_to_idx attributes\n",
        "    self.classes, self.class_to_idx = find_classes_with_indices(targ_dir)\n",
        "\n",
        "  # 4. Create a function to load images\n",
        "  def load_image(self, index: int) -> Image.Image:\n",
        "    \"Opens an image via a path and returns it.\"\n",
        "    image_path = self.paths[index]\n",
        "    return Image.open(image_path)\n",
        "\n",
        "  # 5. Overwrite __len__()\n",
        "  def __len__(self) -> int:\n",
        "    \"Returns the total number of samples.\"\n",
        "    return len(self.paths)\n",
        "\n",
        "  # 6. Overwrite __getitem__() method to return a particular sample\n",
        "  def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
        "    \"Returns one sample of data, data and label (X, y).\"\n",
        "    img = self.load_image(index)\n",
        "    class_name = self.paths[index].parent.name # expects path in format: data_folder/class_name/image.jpg\n",
        "    class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "    # Transform if necessary\n",
        "    if self.transform:\n",
        "      return self.transform(img), class_idx # return data, label (X, y)\n",
        "    else:\n",
        "      return img, class_idx # return untransformed image and label"
      ],
      "metadata": {
        "id": "rBM794sA_bwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FcxPy0huLXCL",
        "outputId": "db43b668-80b5-4014-ba73-04f27679e86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fpr36pE_5qIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to script RNN_EffNet_Code.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxle5AO2A7Tz",
        "outputId": "de235880-7e96-4f09-d60b-c22b8a44616d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern 'RNN_EffNet_Code.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--coalesce-streams\n",
            "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    Overwrite base name use for output files.\n",
            "                Supports pattern replacements '{notebook_name}'.\n",
            "    Default: '{notebook_name}'\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv RNN_EffNet_Code.py /content/Arabic-OCR/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c_LLPM5BcyC",
        "outputId": "f786e216-5841-465f-8f01-0ca3248e6bd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'RNN_EffNet_Code.py': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwwFcNGdCdiB",
        "outputId": "d860dedf-a16c-4699-84ae-8c948ffc6373"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "character_segmentation.py  feature_extraction.py  output\t    test      utilities.py\n",
            "dataset.py\t\t   models\t\t  preprocessing.py  train.py\n",
            "edit.py\t\t\t   OCR.py\t\t  segmentation.py   truth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PC55MuWCk26",
        "outputId": "3a9d250c-9398-4f33-dee9-a48a4bc36285"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VGS1i8xEybv",
        "outputId": "3e8b59a1-f9d5-41a9-c0f2-4c745e83f148"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 00_pytorch_fundmintals.ipynb\t\t\t LSTM_EffNet_Code.ipynb\n",
            " Admin_LSTM_Training.ipynb\t\t\t LSTM_EffNet_EarlyStopping_Code.ipynb\n",
            " AOCR_meeting_5-9-24.pptx\t\t\t MINSTsuryaOCR.py\n",
            " AOCRtrain.ipynb\t\t\t\t MINSTTesting.ipynb\n",
            "'Copy of functionizedEMINST.ipynb'\t\t paragraphGen.ipynb\n",
            "'Copy of PyTorch Lightning Introduction.ipynb'\t PyResNetAHCD.ipynb\n",
            "'Copy of TRAINING_A_CLASSIFIER_tutorial'\t ResNet_Dhad.ipynb\n",
            " Cor_EffNet_LSTM_Dhad.ipynb\t\t\t ResNetTraining.ipynb\n",
            " Counter_Output.ipynb\t\t\t\t ResNetusingAHCD.ipynb\n",
            " Counter_Project.ipynb\t\t\t\t RNN_EffNet_Code.ipynb\n",
            " DatasetGenerator.ipynb\t\t\t\t RNN_EffNet_Code.txt\n",
            " DenseNet.ipynb\t\t\t\t\t Segmentation_Code.ipynb\n",
            " DensNet_Dhad.ipynb\t\t\t\t SuryaOCRAHCD.ipynb\n",
            " Easy_OCR_Test.ipynb\t\t\t\t SuryaOCR.ipynb\n",
            " EfficientNet2.ipynb\t\t\t\t TinyVGGusingAHCD.ipynb\n",
            " EfficientNetAHCD.ipynb\t\t\t\t TinyVGGusingDhad.ipynb\n",
            " EfficientNet_Dhad.ipynb\t\t\t TRAINING_A_CLASSIFIER_tutorial.ipynb\n",
            " EfficientNet_Hijja.ipynb\t\t\t TrainingAhcd1.ipynb\n",
            " EffNet_LSTM_AOCR.ipynb\t\t\t\t Training_AOCR_Classifier.ipynb\n",
            " FunctionizedAhcd1.ipynb\t\t\t TrainingAOCR.ipynb\n",
            " functionizedEMINST.ipynb\t\t\t TrainingEMNISTdataset.ipynb\n",
            " HusseinYoussef.ipynb\t\t\t\t TrainingMINSTdataset.ipynb\n",
            " ImageSegmentation.ipynb\t\t\t Untitled0.ipynb\n",
            " KaggleWithGoColab.ipynb\t\t\t Untitled1.ipynb\n",
            " LettersCounter_2.ipynb\t\t\t\t Updated_RNN_EffNet_Code.ipynb\n",
            " lightningTest.ipynb\t\t\t\t VGGusingAHCD1.ipynb\n",
            " LSTM_AOCR.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to script RNN_EffNet_Code.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbxtV9naE5M4",
        "outputId": "de28fdda-6570-415d-fe58-bacb555e629a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook RNN_EffNet_Code.ipynb to script\n",
            "[NbConvertApp] Writing 66809 bytes to RNN_EffNet_Code.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv RNN_EffNet_Code.txt RNN_EffNet_Code.py"
      ],
      "metadata": {
        "id": "G5oKTrUUFtqJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LZt80v5F3pM",
        "outputId": "448769fd-963c-42ce-d0a1-c7d5a821722b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 00_pytorch_fundmintals.ipynb\t\t\t LSTM_EffNet_Code.ipynb\n",
            " Admin_LSTM_Training.ipynb\t\t\t LSTM_EffNet_EarlyStopping_Code.ipynb\n",
            " AOCR_meeting_5-9-24.pptx\t\t\t MINSTsuryaOCR.py\n",
            " AOCRtrain.ipynb\t\t\t\t MINSTTesting.ipynb\n",
            "'Copy of functionizedEMINST.ipynb'\t\t paragraphGen.ipynb\n",
            "'Copy of PyTorch Lightning Introduction.ipynb'\t PyResNetAHCD.ipynb\n",
            "'Copy of TRAINING_A_CLASSIFIER_tutorial'\t ResNet_Dhad.ipynb\n",
            " Cor_EffNet_LSTM_Dhad.ipynb\t\t\t ResNetTraining.ipynb\n",
            " Counter_Output.ipynb\t\t\t\t ResNetusingAHCD.ipynb\n",
            " Counter_Project.ipynb\t\t\t\t RNN_EffNet_Code.ipynb\n",
            " DatasetGenerator.ipynb\t\t\t\t RNN_EffNet_Code.py\n",
            " DenseNet.ipynb\t\t\t\t\t Segmentation_Code.ipynb\n",
            " DensNet_Dhad.ipynb\t\t\t\t SuryaOCRAHCD.ipynb\n",
            " Easy_OCR_Test.ipynb\t\t\t\t SuryaOCR.ipynb\n",
            " EfficientNet2.ipynb\t\t\t\t TinyVGGusingAHCD.ipynb\n",
            " EfficientNetAHCD.ipynb\t\t\t\t TinyVGGusingDhad.ipynb\n",
            " EfficientNet_Dhad.ipynb\t\t\t TRAINING_A_CLASSIFIER_tutorial.ipynb\n",
            " EfficientNet_Hijja.ipynb\t\t\t TrainingAhcd1.ipynb\n",
            " EffNet_LSTM_AOCR.ipynb\t\t\t\t Training_AOCR_Classifier.ipynb\n",
            " FunctionizedAhcd1.ipynb\t\t\t TrainingAOCR.ipynb\n",
            " functionizedEMINST.ipynb\t\t\t TrainingEMNISTdataset.ipynb\n",
            " HusseinYoussef.ipynb\t\t\t\t TrainingMINSTdataset.ipynb\n",
            " ImageSegmentation.ipynb\t\t\t Untitled0.ipynb\n",
            " KaggleWithGoColab.ipynb\t\t\t Untitled1.ipynb\n",
            " LettersCounter_2.ipynb\t\t\t\t Updated_RNN_EffNet_Code.ipynb\n",
            " lightningTest.ipynb\t\t\t\t VGGusingAHCD1.ipynb\n",
            " LSTM_AOCR.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv RNN_EffNet_Code.py /content/Arabic-OCR/src"
      ],
      "metadata": {
        "id": "XTPfezdrGHi0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/Arabic-OCR/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RtioAXYGVcF",
        "outputId": "c9f10223-e318-479c-83b2-acd4900799de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "character_segmentation.py  feature_extraction.py  output\t      segmentation.py  truth\n",
            "dataset.py\t\t   models\t\t  preprocessing.py    test\t       utilities.py\n",
            "edit.py\t\t\t   OCR.py\t\t  RNN_EffNet_Code.py  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNetRNNModel(nn.Module):\n",
        "    def __init__(self, num_classes, rnn_hidden_size=128, num_rnn_layers=1, nonlinearity='tanh'): #Added the nonlinearity argument to choose between tanh and relu (default is tanh).\n",
        "        super(EfficientNetRNNModel, self).__init__()\n",
        "\n",
        "        # Load EfficientNet B0 as feature extractor\n",
        "        efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Remove the last fully connected layer, we only need the feature extractor\n",
        "        self.feature_extractor = nn.Sequential(*list(efficientnet.children())[:-2])\n",
        "\n",
        "        # RNN layers\n",
        "        self.rnn = nn.RNN(input_size=1280, hidden_size=rnn_hidden_size, num_layers=num_rnn_layers,\n",
        "                          batch_first=True, nonlinearity=nonlinearity)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(rnn_hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # EfficientNet feature extractor\n",
        "        x = self.feature_extractor(x)\n",
        "\n",
        "        # Reshape for RNN: (batch_size, channels, height, width) -> (batch_size, channels, height * width)\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        x = x.view(batch_size, channels, height * width)  # (batch_size, channels, height * width)\n",
        "\n",
        "        # Transpose the dimensions to match RNN input: (batch_size, channels, height * width) -> (batch_size, height * width, channels)\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, height * width, channels)\n",
        "\n",
        "        # RNN forward pass\n",
        "        x, _ = self.rnn(x)  # x: (batch_size, height * width, hidden_size)\n",
        "\n",
        "        # Select the last time step output\n",
        "        x = x[:, -1, :]  # (batch_size, hidden_size)\n",
        "\n",
        "        # Fully connected layer to get final class scores\n",
        "        x = self.fc(x)  # (batch_size, num_classes)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "uUWPRK_aWbo1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model          ##ordered_class_list##\n",
        "model = EfficientNetRNNModel(len(class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LZeEHXiydoA",
        "outputId": "3189ee2c-adce-4ddb-ebac-f6116cbb1e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 156MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ],
      "metadata": {
        "id": "bLNlgbbqo7gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
        "summary(model,\n",
        "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMTgYIQVpXcC",
        "outputId": "8be41ce9-5079-4bbb-d306-01b25db4296d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================================================================================================\n",
              "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
              "=================================================================================================================================================\n",
              "EfficientNetRNNModel (EfficientNetRNNModel)                       [32, 3, 224, 224]    [32, 29]             37,149               True\n",
              "├─Sequential (feature_extractor)                                  [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n",
              "│    └─Sequential (0)                                             [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n",
              "│    │    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   928                  True\n",
              "│    │    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n",
              "│    │    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     16,714               True\n",
              "│    │    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     46,640               True\n",
              "│    │    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     242,930              True\n",
              "│    │    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    543,148              True\n",
              "│    │    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      2,026,348            True\n",
              "│    │    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n",
              "│    │    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     412,160              True\n",
              "├─RNN (rnn)                                                       [32, 49, 1280]       [32, 49, 128]        180,480              True\n",
              "├─Linear (fc)                                                     [32, 128]            [32, 29]             3,741                True\n",
              "=================================================================================================================================================\n",
              "Total params: 4,228,918\n",
              "Trainable params: 4,228,918\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 12.59\n",
              "=================================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3453.70\n",
              "Params size (MB): 16.77\n",
              "Estimated Total Size (MB): 3489.74\n",
              "================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train_step()\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer:torch.optim.Optimizer,\n",
        "               device=device):\n",
        "  # Put the model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Send data to the target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X) # output model logits\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate accuracy metric\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class==y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "ROLVflxxNCDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a test step\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device=device):\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0,  0\n",
        "\n",
        "  # Turn on inference mode\n",
        "  with torch.inference_mode():\n",
        "    # Loop through DataLoader batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to the target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X)\n",
        "\n",
        "      # 2. Calculate the loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate the accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n"
      ],
      "metadata": {
        "id": "cEMSPLiANHTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "# 1. Create a train function that takes in various model parameters + optimizer + dataloaders + loss function\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader,\n",
        "          test_dataloader,\n",
        "          optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5,\n",
        "          device=device):\n",
        "\n",
        "    # 2. Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []}\n",
        "\n",
        "    # 3. Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           dataloader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer,\n",
        "                                           device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "                                        dataloader=test_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        device=device)\n",
        "\n",
        "        # 4. Print out what's happening\n",
        "        print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
        "\n",
        "        # 5. Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # 6. Calculate and print the average metrics\n",
        "    avg_train_loss = sum(results[\"train_loss\"]) / len(results[\"train_loss\"])\n",
        "    avg_train_acc = sum(results[\"train_acc\"]) / len(results[\"train_acc\"])\n",
        "    avg_test_loss = sum(results[\"test_loss\"]) / len(results[\"test_loss\"])\n",
        "    avg_test_acc = sum(results[\"test_acc\"]) / len(results[\"test_acc\"])\n",
        "\n",
        "    print(\"\\n--- Average Metrics After Training ---\")\n",
        "    print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Average Train Accuracy: {avg_train_acc:.4f}\")\n",
        "    print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Average Test Accuracy: {avg_test_acc:.4f}\")\n",
        "\n",
        "    # 7. Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "853HVXdPOyih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.ASGD(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "9In9JMCRppVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################ the run time take 48m\n",
        "# Set the random seeds\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "results = train(model=model,\n",
        "                       train_dataloader=train_loaderr,\n",
        "                       test_dataloader=test_loaderr,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=30,\n",
        "                       device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "f26c9a782e40417caa4056c29e8a6da2",
            "6dbb0b11dab7413983e9c04ecc7ae8be",
            "6fbccced5c0a41ada65fdbe4063d747a",
            "380ac176dfe9475ca01294a38a0a8e60",
            "6f9b930a6aed45e5ac0dcb419a9e22f1",
            "701301f1f48a47d7b4ee651dab55e214",
            "90e7db91cf7a4f3096872ec77875aa73",
            "74cd5082886d4bdc9f53d5eb6748739c",
            "7e1248b3c10c47d8b5bd10bc1046a041",
            "0009f88809e94bba9e5596edb165ebf8",
            "aeac5d0ee49947668d8547239273cb65"
          ]
        },
        "id": "oAXUPwxIpzLg",
        "outputId": "5ff6dd70-b92b-4ba1-fa5f-880da0b3413c"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f26c9a782e40417caa4056c29e8a6da2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train loss: 1.3004 | Train acc: 0.6171 | Test loss: 0.5978 | Test acc: 0.8340\n",
            "Epoch: 1 | Train loss: 0.5529 | Train acc: 0.8390 | Test loss: 0.4704 | Test acc: 0.8691\n",
            "Epoch: 2 | Train loss: 0.4662 | Train acc: 0.8681 | Test loss: 0.3968 | Test acc: 0.8895\n",
            "Epoch: 3 | Train loss: 0.4171 | Train acc: 0.8831 | Test loss: 0.3776 | Test acc: 0.8955\n",
            "Epoch: 4 | Train loss: 0.3799 | Train acc: 0.8941 | Test loss: 0.4314 | Test acc: 0.8800\n",
            "Epoch: 5 | Train loss: 0.3842 | Train acc: 0.8927 | Test loss: 0.4337 | Test acc: 0.8796\n",
            "Epoch: 6 | Train loss: 0.3431 | Train acc: 0.9005 | Test loss: 0.4047 | Test acc: 0.8874\n",
            "Epoch: 7 | Train loss: 0.3347 | Train acc: 0.9055 | Test loss: 0.3687 | Test acc: 0.9019\n",
            "Epoch: 8 | Train loss: 0.3169 | Train acc: 0.9088 | Test loss: 0.3809 | Test acc: 0.8958\n",
            "Epoch: 9 | Train loss: 0.3166 | Train acc: 0.9098 | Test loss: 0.3896 | Test acc: 0.8962\n",
            "Epoch: 10 | Train loss: 0.3057 | Train acc: 0.9129 | Test loss: 0.3595 | Test acc: 0.9038\n",
            "Epoch: 11 | Train loss: 0.3018 | Train acc: 0.9141 | Test loss: 0.3294 | Test acc: 0.9118\n",
            "Epoch: 12 | Train loss: 0.3076 | Train acc: 0.9119 | Test loss: 0.3935 | Test acc: 0.8983\n",
            "Epoch: 13 | Train loss: 0.2979 | Train acc: 0.9169 | Test loss: 0.3749 | Test acc: 0.9060\n",
            "Epoch: 14 | Train loss: 0.2881 | Train acc: 0.9183 | Test loss: 0.3449 | Test acc: 0.9061\n",
            "Epoch: 15 | Train loss: 0.2927 | Train acc: 0.9164 | Test loss: 0.3172 | Test acc: 0.9171\n",
            "Epoch: 16 | Train loss: 0.2669 | Train acc: 0.9209 | Test loss: 0.3343 | Test acc: 0.9114\n",
            "Epoch: 17 | Train loss: 0.2540 | Train acc: 0.9284 | Test loss: 0.3070 | Test acc: 0.9175\n",
            "Epoch: 18 | Train loss: 0.2425 | Train acc: 0.9308 | Test loss: 0.3284 | Test acc: 0.9168\n",
            "Epoch: 19 | Train loss: 0.2334 | Train acc: 0.9312 | Test loss: 0.2975 | Test acc: 0.9200\n",
            "Epoch: 20 | Train loss: 0.2532 | Train acc: 0.9279 | Test loss: 0.3741 | Test acc: 0.9046\n",
            "Epoch: 21 | Train loss: 0.2423 | Train acc: 0.9312 | Test loss: 0.3153 | Test acc: 0.9164\n",
            "Epoch: 22 | Train loss: 0.2233 | Train acc: 0.9348 | Test loss: 0.3023 | Test acc: 0.9215\n",
            "Epoch: 23 | Train loss: 0.2355 | Train acc: 0.9316 | Test loss: 0.3386 | Test acc: 0.9160\n",
            "Epoch: 24 | Train loss: 0.2315 | Train acc: 0.9327 | Test loss: 0.3156 | Test acc: 0.9199\n",
            "Epoch: 25 | Train loss: 0.2220 | Train acc: 0.9357 | Test loss: 0.3123 | Test acc: 0.9211\n",
            "Epoch: 26 | Train loss: 0.2131 | Train acc: 0.9386 | Test loss: 0.3452 | Test acc: 0.9135\n",
            "Epoch: 27 | Train loss: 0.2210 | Train acc: 0.9356 | Test loss: 0.3059 | Test acc: 0.9207\n",
            "Epoch: 28 | Train loss: 0.2117 | Train acc: 0.9387 | Test loss: 0.3157 | Test acc: 0.9175\n",
            "Epoch: 29 | Train loss: 0.2075 | Train acc: 0.9382 | Test loss: 0.3143 | Test acc: 0.9195\n",
            "\n",
            "--- Average Metrics After Training ---\n",
            "Average Train Loss: 0.3288\n",
            "Average Train Accuracy: 0.9055\n",
            "Average Test Loss: 0.3626\n",
            "Average Test Accuracy: 0.9036\n",
            "[INFO] Total training time: 3599.428 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcbFUpSiCFHl",
        "outputId": "66113b7c-5dbc-4975-e3fd-a4a6d5ebe2fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"/content/drive/MyDrive/Saved_Models/new_save_model_rnn_eff_92.pth\""
      ],
      "metadata": {
        "id": "CWTbEmL4pzy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),FILE)"
      ],
      "metadata": {
        "id": "AJt2UDAApdSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming `model` is your trained model\n",
        "model_path = \"/content/drive/MyDrive/Saved_Models/efficientnet_rnn_model_92.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "WGb66ei6suEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, save the model\n",
        "def save_model(model, path='/content/drive/MyDrive/Saved_Models/efficientnet_rnn_model_95.pth'):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ],
      "metadata": {
        "id": "1WyEqUKgMFNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To load the saved model\n",
        "def load_model(path='efficientnet_rnn_model.pth'):\n",
        "    model = EfficientNetRNNModel(num_classes=len(class_names))\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "sP6Rui0FDjGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, '/content/drive/MyDrive/Saved_Models/efficientnet_rnn_model_95.pth')"
      ],
      "metadata": {
        "id": "RWOOMgAgMLQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "save_model(model, '/content/drive/MyDrive/Saved_Models/efficientnet_rnn_model_eval_95.pth')"
      ],
      "metadata": {
        "id": "YdpJjjZWxxbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\n",
        "try:\n",
        "    from helper_functions import plot_loss_curves\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n",
        "    with open(\"helper_functions.py\", \"wb\") as f:\n",
        "        import requests\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "        f.write(request.content)\n",
        "    from helper_functions import plot_loss_curves\n",
        "\n",
        "# Plot the loss curves of our model\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "uNCBbqueqx60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from PIL import Image\n",
        "\n",
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str],\n",
        "                        image_size: Tuple[int, int] = (224, 224),\n",
        "                        transform: torchvision.transforms = None,\n",
        "                        device: torch.device = device):\n",
        "\n",
        "    # Open image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Convert grayscale image to RGB if necessary\n",
        "    if img.mode != \"RGB\":\n",
        "        img = img.convert(\"RGB\")\n",
        "\n",
        "    # Create transformation for image\n",
        "    if transform is not None:\n",
        "        image_transform = transform\n",
        "    else:\n",
        "        image_transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    # Predict on image\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        transformed_image = image_transform(img).unsqueeze(dim=0)\n",
        "        target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "    # Convert logits -> prediction probabilities\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # Convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # Plot image with predicted label and probability\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Pred: {class_names[target_image_pred_label.item()]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "GAcrJocxrOTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random list of image paths from test set\n",
        "import random\n",
        "num_images_to_plot = 3\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.png\")) # get list all image paths from test data\n",
        "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
        "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
        "\n",
        "# Make predictions on and plot the images\n",
        "for image_path in test_image_path_sample:\n",
        "    pred_and_plot_image(model=model,\n",
        "                        image_path=image_path,\n",
        "                        class_names=class_names,\n",
        "                        #transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))"
      ],
      "metadata": {
        "id": "6zF7nihtrW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientNetRNNModel(num_classes=29)  # Make sure to match the num_classes with your trained model\n",
        "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/Saved_Models/efficientnet_rnn_model_95.pth\", strict=False))  # Load your trained model\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "a_PFKpqrh2ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the same transforms used during training\n",
        "# Load an image and apply transformations\n",
        "image = Image.open(\"/content/drive/MyDrive/Arabic_OCR_Tests/char_6.png\")\n",
        "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n"
      ],
      "metadata": {
        "id": "VT30wzBci9Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():  # Disable gradient calculations for inference\n",
        "    output = model(input_tensor)  # Forward pass\n"
      ],
      "metadata": {
        "id": "szdvccyxjjML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted class index (highest score for each input in the batch)\n",
        "predicted_index = torch.argmax(output, dim=1).item()  # Use .item() to get Python int if single prediction\n",
        "print(\"Predicted class index:\", predicted_index)"
      ],
      "metadata": {
        "id": "3vssAZLHhnRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Function to preprocess the image and create a binary representation\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
        "    _, binary_img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)  # Apply binary threshold\n",
        "    return binary_img\n",
        "\n",
        "# Function to perform segmentation of characters and save each segment as an image\n",
        "def segment_and_save_characters(binary_img, output_folder='segmented_characters'):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)  # Create the output directory if it doesn't exist\n",
        "\n",
        "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    character_count = 0\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        if w > 5 and h > 5:  # Filter out small noise\n",
        "            char_img = binary_img[y:y + h, x:x + w]  # Crop character from image\n",
        "            char_img_resized = cv2.resize(char_img, (28, 28))  # Resize for consistency\n",
        "            character_count += 1\n",
        "            char_filename = os.path.join(output_folder, f'char_{character_count}.png')\n",
        "            cv2.imwrite(char_filename, char_img_resized)  # Save the character image\n",
        "            print(f'Saved character {character_count} as {char_filename}')\n",
        "\n",
        "# Main function to process the image and perform segmentation\n",
        "def process_image_and_save_segments(image_path, output_folder='segmented_characters'):\n",
        "    binary_img = preprocess_image(image_path)\n",
        "    segment_and_save_characters(binary_img, output_folder)\n",
        "\n",
        "# Path to the Arabic paragraph image\n",
        "image_path = '/content/drive/MyDrive/Arabic_OCR_Tests/arabic_paragraph_original.png'  # Update with your image path\n",
        "process_image_and_save_segments(image_path, output_folder='segmented_characters')\n"
      ],
      "metadata": {
        "id": "IoTAwPlYsRGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping model class names to Arabic characters\n",
        "class_name_to_arabic = {\n",
        "    '1-alif': 'ا', '2-baa': 'ب', '3-taa': 'ت', '4-thaa': 'ث', '5-jeem': 'ج',\n",
        "    '6-h_aa': 'ح', '7-khaa': 'خ', '8-dal': 'د', '9-thal': 'ذ', '10-raa': 'ر',\n",
        "    '11-zeyn': 'ز', '12-seen': 'س', '13-sheen': 'ش', '14-sad': 'ص', '15-dhad': 'ض',\n",
        "    '16-t_aa': 'ط', '17-th_aa': 'ظ', '18-ayen': 'ع', '19-ghayen': 'غ',\n",
        "    '20-faa': 'ف', '21-ghaf': 'ق', '22-kaf': 'ك', '23-lam': 'ل',\n",
        "    '24-meem': 'م', '25-noon': 'ن', '26-haa': 'ه', '27-waw': 'و', '28-yaa': 'ي',\n",
        "    '29-hamzah': 'ء'\n",
        "}\n"
      ],
      "metadata": {
        "id": "0gmhdFLHwO00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert index to class name and then to Arabic character\n",
        "predicted_class_name = list(class_name_to_arabic.keys())[predicted_index]\n",
        "predicted_arabic_character = class_name_to_arabic[predicted_class_name]\n",
        "print(\"Predicted Arabic Character:\", predicted_arabic_character)\n"
      ],
      "metadata": {
        "id": "7MTDG5r_jstV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample output from model prediction (this should be actual model output in your case)\n",
        "predicted_indices = [0, 1, 2, 3, 4]  # Replace this with actual model prediction output\n",
        "\n",
        "# Convert indices to class names, then to Arabic characters\n",
        "predicted_classes = [list(class_name_to_arabic.keys())[i] for i in predicted_indices]\n",
        "predicted_arabic_chars = [class_name_to_arabic[class_name] for class_name in predicted_classes]\n",
        "\n",
        "print(\"Predicted Arabic Characters:\", predicted_arabic_chars)\n"
      ],
      "metadata": {
        "id": "dvY_eQvywgRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "# Function to preprocess the image and create a binary representation\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
        "    _, binary_img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)  # Apply binary threshold\n",
        "    return binary_img\n",
        "\n",
        "# Function to perform segmentation of characters and save each segment as an image\n",
        "def segment_and_save_characters(binary_img, output_folder='segmented_characters'):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)  # Create the output directory if it doesn't exist\n",
        "\n",
        "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    character_count = 0\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        if w > 5 and h > 5:  # Filter out small noise\n",
        "            char_img = binary_img[y:y + h, x:x + w]  # Crop character from image\n",
        "            char_img_resized = cv2.resize(char_img, (28, 28))  # Resize for consistency\n",
        "            character_count += 1\n",
        "            char_filename = os.path.join(output_folder, f'char_{character_count}.png')\n",
        "            cv2.imwrite(char_filename, char_img_resized)  # Save the character image\n",
        "            print(f'Saved character {character_count} as {char_filename}')\n",
        "\n",
        "# Main function to process the image and perform segmentation\n",
        "def process_image_and_save_segments(image_path, output_folder='segmented_characters'):\n",
        "    binary_img = preprocess_image(image_path)\n",
        "    segment_and_save_characters(binary_img, output_folder)\n",
        "\n",
        "# Path to the Arabic paragraph image\n",
        "image_path = '/content/drive/MyDrive/Arabic_OCR_Tests/arabic_paragraph_original.png'  # Replace with your image path\n",
        "process_image_and_save_segments(image_path, output_folder='segmented_characters')\n"
      ],
      "metadata": {
        "id": "I-3ddCLty2BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import csv\n",
        "\n",
        "class ArabicLetterDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
        "            next(f)  # Skip header\n",
        "            for line in f:\n",
        "                image_path, label = line.strip().split(',')\n",
        "                self.data.append(image_path)\n",
        "                self.labels.append(int(label))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(self.data[idx], cv2.IMREAD_GRAYSCALE)\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "whtNROWX2-cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Specify the image path in your Google Drive\n",
        "image_path = \"/content/drive/MyDrive/Arabic_OCR_Tests/char_6.png\"\n",
        "\n",
        "# Specify the path where you want to save the downloaded image in Colab\n",
        "save_directory = '/content/'\n",
        "save_filename = 'char_6.png'\n",
        "\n",
        "# Download and save the image\n",
        "transform = transforms.ToTensor()\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image)\n",
        "\n",
        "save_path = os.path.join(save_directory, save_filename)\n",
        "torchvision.utils.save_image(image_tensor, save_path)"
      ],
      "metadata": {
        "id": "dhZK6_dQyRw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path of the image\n",
        "custom_image_path = \"/content/char_6.png\"\n",
        "\n",
        "# Check if the image file exists\n",
        "if os.path.isfile(custom_image_path):\n",
        "    # Read in custom image\n",
        "    custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "    # Print out image data\n",
        "    print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "    print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "    print(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n",
        "else:\n",
        "    print(f\"The image file at {custom_image_path} does not exist.\")"
      ],
      "metadata": {
        "id": "nbVyy02ZyXFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ],
      "metadata": {
        "id": "e4oUMLmU1LaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255.\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ],
      "metadata": {
        "id": "0wNV8fN61Rxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot custom image\n",
        "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
        "plt.title(f\"Image shape: {custom_image.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "Mdbcllol1DDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transform pipleine to resize image\n",
        "custom_image_transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "\n",
        "])\n",
        "# Transform target image\n",
        "custom_image_pil = transforms.ToPILImage()(custom_image)  # Convert tensor to PIL.Image\n",
        "custom_image_transformed = custom_image_transform(custom_image_pil)\n",
        "\n",
        "# Transform target image\n",
        "#custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out original shape and new shape\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"New shape: {custom_image_transformed.shape}\")"
      ],
      "metadata": {
        "id": "ynNYGKWg1m6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    # Add an extra dimension to image\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "\n",
        "    # Print out different shapes\n",
        "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
        "\n",
        "    # Make a prediction on image with an extra dimension\n",
        "    custom_image_pred = model(custom_image_transformed.unsqueeze(dim=0).to(device))"
      ],
      "metadata": {
        "id": "aDH28W7z5d38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_pred"
      ],
      "metadata": {
        "id": "ND7kC-ZH5p0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ],
      "metadata": {
        "id": "KGMl1rhB5ug4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ],
      "metadata": {
        "id": "9XqxHeOO52_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The values of the prediction probabilities are quite similar\n",
        "custom_image_pred_probs"
      ],
      "metadata": {
        "id": "McZonbN458Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                        image_path=custom_image_path,\n",
        "                        class_names=class_names,\n",
        "                        #transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))"
      ],
      "metadata": {
        "id": "MagOHyUDEnlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another testing for outside image"
      ],
      "metadata": {
        "id": "9WOWYVDPALRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the image path in your Google Drive\n",
        "image_path = \"/content/drive/MyDrive/Arabic_OCR_Tests/char_1.jpg\"\n",
        "\n",
        "# Specify the path where you want to save the downloaded image in Colab\n",
        "save_directory = '/content/'\n",
        "save_filename = 'char_1.jpg'\n",
        "\n",
        "# Download and save the image\n",
        "transform = transforms.ToTensor()\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image)\n",
        "\n",
        "save_path = os.path.join(save_directory, save_filename)\n",
        "torchvision.utils.save_image(image_tensor, save_path)"
      ],
      "metadata": {
        "id": "OTZhZQ6vAIY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path of the image\n",
        "custom_image_path = \"/content/char_1.jpg\"\n",
        "\n",
        "# Check if the image file exists\n",
        "if os.path.isfile(custom_image_path):\n",
        "    # Read in custom image\n",
        "    custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "    # Print out image data\n",
        "    print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "    print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "    print(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n",
        "else:\n",
        "    print(f\"The image file at {custom_image_path} does not exist.\")"
      ],
      "metadata": {
        "id": "hAyB1IwtAIY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ],
      "metadata": {
        "id": "_1xeSzeIAIY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255.\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ],
      "metadata": {
        "id": "p9LS7jRlAIY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot custom image\n",
        "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
        "plt.title(f\"Image shape: {custom_image.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "R8IGZ6LRAIY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transform pipleine to resize image\n",
        "custom_image_transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "\n",
        "])\n",
        "# Transform target image\n",
        "custom_image_pil = transforms.ToPILImage()(custom_image)  # Convert tensor to PIL.Image\n",
        "custom_image_transformed = custom_image_transform(custom_image_pil)\n",
        "\n",
        "# Transform target image\n",
        "#custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out original shape and new shape\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"New shape: {custom_image_transformed.shape}\")"
      ],
      "metadata": {
        "id": "-pth3ySEAIY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    # Add an extra dimension to image\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "\n",
        "    # Print out different shapes\n",
        "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
        "\n",
        "    # Make a prediction on image with an extra dimension\n",
        "    custom_image_pred = model(custom_image_transformed.unsqueeze(dim=0).to(device))"
      ],
      "metadata": {
        "id": "MEAG0D_2AIY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_pred"
      ],
      "metadata": {
        "id": "9pP3qt20AIY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ],
      "metadata": {
        "id": "dyGgbnP_AIY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ],
      "metadata": {
        "id": "c9rOQ8G2AIZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                        image_path=custom_image_path,\n",
        "                        class_names=class_names,\n",
        "                        #transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))\n"
      ],
      "metadata": {
        "id": "09ukDRiiDHr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E89dCO-QIrsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the image path in your Google Drive\n",
        "image_path = \"/content/drive/MyDrive/Arabic_OCR_Tests/char_2.png\"\n",
        "\n",
        "# Specify the path where you want to save the downloaded image in Colab\n",
        "save_directory = '/content/'\n",
        "save_filename = 'char_2.png'\n",
        "\n",
        "# Download and save the image\n",
        "transform = transforms.ToTensor()\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image)\n",
        "\n",
        "save_path = os.path.join(save_directory, save_filename)\n",
        "torchvision.utils.save_image(image_tensor, save_path)"
      ],
      "metadata": {
        "id": "RPNsrrvrI-sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path of the image\n",
        "custom_image_path = \"/content/char_2.png\"\n",
        "\n",
        "# Check if the image file exists\n",
        "if os.path.isfile(custom_image_path):\n",
        "    # Read in custom image\n",
        "    custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "    # Print out image data\n",
        "    print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "    print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "    print(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n",
        "else:\n",
        "    print(f\"The image file at {custom_image_path} does not exist.\")"
      ],
      "metadata": {
        "id": "jEia6FmpI-se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ],
      "metadata": {
        "id": "Bb74UcwII-se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255.\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ],
      "metadata": {
        "id": "L4vagA2OI-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot custom image\n",
        "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
        "plt.title(f\"Image shape: {custom_image.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "kVTO608JI-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transform pipleine to resize image\n",
        "custom_image_transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "\n",
        "])\n",
        "# Transform target image\n",
        "custom_image_pil = transforms.ToPILImage()(custom_image)  # Convert tensor to PIL.Image\n",
        "custom_image_transformed = custom_image_transform(custom_image_pil)\n",
        "\n",
        "# Transform target image\n",
        "#custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out original shape and new shape\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"New shape: {custom_image_transformed.shape}\")"
      ],
      "metadata": {
        "id": "7HX-mgffI-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    # Add an extra dimension to image\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "\n",
        "    # Print out different shapes\n",
        "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
        "\n",
        "    # Make a prediction on image with an extra dimension\n",
        "    custom_image_pred = model(custom_image_transformed.unsqueeze(dim=0).to(device))"
      ],
      "metadata": {
        "id": "FVz7wYy2I-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_pred"
      ],
      "metadata": {
        "id": "eQoZ6NXWI-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ],
      "metadata": {
        "id": "Vtr-vDaKI-sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ],
      "metadata": {
        "id": "Ww3oHDYhI-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                        image_path=custom_image_path,\n",
        "                        class_names=class_names,\n",
        "                        #transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))\n"
      ],
      "metadata": {
        "id": "gIWPOHzBI-sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-fDfSLADJiSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the image path in your Google Drive\n",
        "image_path = \"/content/drive/MyDrive/Arabic_OCR_Tests/char_16.png\"\n",
        "\n",
        "# Specify the path where you want to save the downloaded image in Colab\n",
        "save_directory = '/content/'\n",
        "save_filename = 'char_16.png'\n",
        "\n",
        "# Download and save the image\n",
        "transform = transforms.ToTensor()\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image)\n",
        "\n",
        "save_path = os.path.join(save_directory, save_filename)\n",
        "torchvision.utils.save_image(image_tensor, save_path)"
      ],
      "metadata": {
        "id": "2IndqgmFJimD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path of the image\n",
        "custom_image_path = \"/content/char_16.png\"\n",
        "\n",
        "# Check if the image file exists\n",
        "if os.path.isfile(custom_image_path):\n",
        "    # Read in custom image\n",
        "    custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "    # Print out image data\n",
        "    print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "    print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "    print(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n",
        "else:\n",
        "    print(f\"The image file at {custom_image_path} does not exist.\")"
      ],
      "metadata": {
        "id": "zqzdK5W2JimE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ],
      "metadata": {
        "id": "6_ALn9deJimE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255.\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ],
      "metadata": {
        "id": "9xZzS65ZJimF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot custom image\n",
        "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
        "plt.title(f\"Image shape: {custom_image.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "oBzQRcQIJimF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transform pipleine to resize image\n",
        "custom_image_transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "\n",
        "])\n",
        "# Transform target image\n",
        "custom_image_pil = transforms.ToPILImage()(custom_image)  # Convert tensor to PIL.Image\n",
        "custom_image_transformed = custom_image_transform(custom_image_pil)\n",
        "\n",
        "# Transform target image\n",
        "#custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out original shape and new shape\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"New shape: {custom_image_transformed.shape}\")"
      ],
      "metadata": {
        "id": "V-ri85nGJimG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    # Add an extra dimension to image\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "\n",
        "    # Print out different shapes\n",
        "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
        "\n",
        "    # Make a prediction on image with an extra dimension\n",
        "    custom_image_pred = model(custom_image_transformed.unsqueeze(dim=0).to(device))"
      ],
      "metadata": {
        "id": "05aZ8NAlJimG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_pred"
      ],
      "metadata": {
        "id": "XpafT6znJimG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ],
      "metadata": {
        "id": "ngFI6QyPJimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ],
      "metadata": {
        "id": "DlXl1CBGJimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                        image_path=custom_image_path,\n",
        "                        class_names=class_names,\n",
        "                        #transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))\n"
      ],
      "metadata": {
        "id": "yrHUq7yaJimI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CCuggZwUmZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the image path in your Google Drive\n",
        "image_path = \"/content/drive/MyDrive/Arabic_OCR_Tests/char_28_c.png\"\n",
        "\n",
        "# Specify the path where you want to save the downloaded image in Colab\n",
        "save_directory = '/content/'\n",
        "save_filename = 'char_28_c.png'\n",
        "\n",
        "# Download and save the image\n",
        "transform = transforms.ToTensor()\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image)\n",
        "\n",
        "save_path = os.path.join(save_directory, save_filename)\n",
        "torchvision.utils.save_image(image_tensor, save_path)"
      ],
      "metadata": {
        "id": "8WPEG-ilUmrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path of the image\n",
        "custom_image_path = \"/content/char_28_c.png\"\n",
        "\n",
        "# Check if the image file exists\n",
        "if os.path.isfile(custom_image_path):\n",
        "    # Read in custom image\n",
        "    custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "    # Print out image data\n",
        "    print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "    print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "    print(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n",
        "else:\n",
        "    print(f\"The image file at {custom_image_path} does not exist.\")"
      ],
      "metadata": {
        "id": "eD2KXmNhUmrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ],
      "metadata": {
        "id": "8iocA9KrUmrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255.\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ],
      "metadata": {
        "id": "JVkBEqo2Umrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot custom image\n",
        "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
        "plt.title(f\"Image shape: {custom_image.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "qwq8OopoUmre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transform pipleine to resize image\n",
        "custom_image_transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "\n",
        "])\n",
        "# Transform target image\n",
        "custom_image_pil = transforms.ToPILImage()(custom_image)  # Convert tensor to PIL.Image\n",
        "custom_image_transformed = custom_image_transform(custom_image_pil)\n",
        "\n",
        "# Transform target image\n",
        "#custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out original shape and new shape\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"New shape: {custom_image_transformed.shape}\")"
      ],
      "metadata": {
        "id": "uFS6oLspUmre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    # Add an extra dimension to image\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "\n",
        "    # Print out different shapes\n",
        "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
        "\n",
        "    # Make a prediction on image with an extra dimension\n",
        "    custom_image_pred = model(custom_image_transformed.unsqueeze(dim=0).to(device))"
      ],
      "metadata": {
        "id": "OY6GhVK8Umrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_pred"
      ],
      "metadata": {
        "id": "ahDI1Lu5Umrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ],
      "metadata": {
        "id": "uo_lzUloUmrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ],
      "metadata": {
        "id": "9jbRk98DUmrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                        image_path=custom_image_path,\n",
        "                        class_names=class_names,\n",
        "                        #transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))\n"
      ],
      "metadata": {
        "id": "BqgKRtgoUmri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NAqbuTtU-HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the image path in your Google Drive\n",
        "image_path = \"/content/drive/MyDrive/Arabic_OCR_Tests/char_25_c.png\"\n",
        "\n",
        "# Specify the path where you want to save the downloaded image in Colab\n",
        "save_directory = '/content/'\n",
        "save_filename = 'char_25_c.png'\n",
        "\n",
        "# Download and save the image\n",
        "transform = transforms.ToTensor()\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image)\n",
        "\n",
        "save_path = os.path.join(save_directory, save_filename)\n",
        "torchvision.utils.save_image(image_tensor, save_path)"
      ],
      "metadata": {
        "id": "A41oFFYnU-d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the path of the image\n",
        "custom_image_path = \"/content/char_25_c.png\"\n",
        "\n",
        "# Check if the image file exists\n",
        "if os.path.isfile(custom_image_path):\n",
        "    # Read in custom image\n",
        "    custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "    # Print out image data\n",
        "    print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "    print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "    print(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n",
        "else:\n",
        "    print(f\"The image file at {custom_image_path} does not exist.\")"
      ],
      "metadata": {
        "id": "qQ8ux2U6U-d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# Read in custom image\n",
        "custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\n",
        "print(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image_uint8.dtype}\")"
      ],
      "metadata": {
        "id": "IdmRawlGU-d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in custom image and convert the tensor values to float32\n",
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "# Divide the image pixel values by 255 to get them between [0, 1]\n",
        "custom_image = custom_image / 255.\n",
        "\n",
        "# Print out image data\n",
        "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
        "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
        "print(f\"Custom image dtype: {custom_image.dtype}\")"
      ],
      "metadata": {
        "id": "D7OqNFhXU-d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot custom image\n",
        "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
        "plt.title(f\"Image shape: {custom_image.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "olxd_yD_U-d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create transform pipleine to resize image\n",
        "custom_image_transform = transforms.Compose([\n",
        "    GrayscaleToRGB(),\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "\n",
        "])\n",
        "# Transform target image\n",
        "custom_image_pil = transforms.ToPILImage()(custom_image)  # Convert tensor to PIL.Image\n",
        "custom_image_transformed = custom_image_transform(custom_image_pil)\n",
        "\n",
        "# Transform target image\n",
        "#custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "# Print out original shape and new shape\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"New shape: {custom_image_transformed.shape}\")"
      ],
      "metadata": {
        "id": "Fj2nFeTtU-eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    # Add an extra dimension to image\n",
        "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
        "\n",
        "    # Print out different shapes\n",
        "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
        "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
        "\n",
        "    # Make a prediction on image with an extra dimension\n",
        "    custom_image_pred = model(custom_image_transformed.unsqueeze(dim=0).to(device))"
      ],
      "metadata": {
        "id": "WFJYh6u6U-eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_pred"
      ],
      "metadata": {
        "id": "Ty1OHl8YU-eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out prediction logits\n",
        "print(f\"Prediction logits: {custom_image_pred}\")\n",
        "\n",
        "# Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
        "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
        "\n",
        "# Convert prediction probabilities -> prediction labels\n",
        "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
        "print(f\"Prediction label: {custom_image_pred_label}\")"
      ],
      "metadata": {
        "id": "715bw8BxU-eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted label\n",
        "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
        "custom_image_pred_class"
      ],
      "metadata": {
        "id": "KZlUoKL4U-eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                        image_path=custom_image_path,\n",
        "                        class_names=class_names,\n",
        "                        #transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))\n"
      ],
      "metadata": {
        "id": "Cz_WWjjaU-eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making a confusion matrix for further prediction evaluation**\n",
        "source code\n",
        "\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/03_pytorch_computer_vision.ipynb#scrollTo=ab108078-6770-4cb9-ac62-a761ff159aba\""
      ],
      "metadata": {
        "id": "PGcX4SV5bmDT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "065b8090-c9c5-43df-b5c1-b45ba33af1be"
      },
      "outputs": [],
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_loaderr, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Do the forward pass\n",
        "    y_logit = model(X)\n",
        "    # Turn predictions from logits -> prediction probabilities -> predictions labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6c0a05d-d3e0-4b86-9ef7-ee6ea5629b07"
      },
      "outputs": [],
      "source": [
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21383f88-a2dd-4678-94c6-479c592da0ab"
      },
      "outputs": [],
      "source": [
        "# Import mlxtend upgraded version\n",
        "import mlxtend\n",
        "print(mlxtend.__version__)\n",
        "assert int(mlxtend.__version__.split(\".\")[1]) >= 19 # should be version 0.19.0 or higher"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors if they are lists\n",
        "if isinstance(y_pred_tensor, list):\n",
        "    y_pred_tensor = torch.tensor(y_pred_tensor)\n",
        "\n",
        "if isinstance(test_dataset.targets, list):\n",
        "    test_dataset.targets = torch.tensor(test_dataset.targets)\n"
      ],
      "metadata": {
        "id": "jtiMKYNdxERl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aed6d76-ad1c-429e-b8e0-c80572e3ebf4"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 2. Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                         target=test_dataset.targets)\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy\n",
        "    class_names=class_names, # turn the row and column labels into class names\n",
        "    figsize=(20, 14)\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjbpUSF5hybP"
      },
      "outputs": [],
      "source": [
        "# Number of classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = EfficientNetRNNModel(num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    y_true_train = []\n",
        "    y_pred_train = []\n",
        "\n",
        "    for images, labels in train_loaderr:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true_train.extend(labels.cpu().numpy())\n",
        "        y_pred_train.extend(predicted.cpu().numpy())\n",
        "\n",
        "    train_loss = running_loss / len(train_loaderr)\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    train_precision = precision_score(y_true_train, y_pred_train, average='macro', zero_division=0)\n",
        "    train_recall = recall_score(y_true_train, y_pred_train, average='macro', zero_division=0)\n",
        "    train_f1 = f1_score(y_true_train, y_pred_train, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "          f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1 Score: {train_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = EfficientNetRNNModel(num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 30\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    y_true_test = []\n",
        "    y_pred_test = []\n",
        "\n",
        "    for images, labels in test_loaderr:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true_test.extend(labels.cpu().numpy())\n",
        "        y_pred_test.extend(predicted.cpu().numpy())\n",
        "\n",
        "    test_loss = running_loss / len(test_loaderr)\n",
        "    test_accuracy = accuracy_score(y_true_test, y_pred_test)\n",
        "    test_precision = precision_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
        "    test_recall = recall_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
        "    ttest_f1 = f1_score(y_true_test, y_pred_test, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n",
        "          f\"Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1 Score: {test_f1:.4f}\")"
      ],
      "metadata": {
        "id": "RIb_65eLZlb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientNetRNNModel(num_classes=len(class_names))\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Saved_Models/efficientnet_rnn_model.pth'))## '/content/drive/MyDrive/RNN_EffNet_Code/'\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "d3kM6aTwtS4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}